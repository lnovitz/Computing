---
title: "HousingModels"
author: "Lianna Novitz"
date: "3/23/2018"
output: html_document
---

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(ggplot2) #tidyverse
library(dplyr) #tidyverse
library(tidyr) #tidyverse
library(readr) #tidyverse
library(gridExtra)
library(GGally) 
library(leaps) 
library(glmnet)

house_data <- read_csv("https://www.dropbox.com/s/lslekih13vae487/house_train_final.csv?dl=1")

#if you don't need a code chunk at all, {r, eval=FALSE, echo=FALSE}
#if you don't need to show the code but need it evaluated, {r, echo=FALSE}
#won't knit if you have ANY errors, be careful
#don't knit with View(dataset), too big, 
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(1:9,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(10:19,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(20:29,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
#```{r, fig.width=12, fig.height=12}
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(30:39,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(40:49,50)) %>% 
  ggpairs()
```

After performing the ggpairs function, I noticed that there are a few variables which have a high correlation with SalePrice. I have graphed these relationships below.
```{r, echo=FALSE}
qualP <-
house_data %>% 
  ggplot(aes(x=OverallQual, y=SalePrice)) +
  geom_point()

buildyearP <-
house_data %>% 
  ggplot(aes(x=YearBuilt, y=SalePrice)) +
  geom_point()

remodyearP <-
house_data %>% 
  ggplot(aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point()

bsmtsfP <-
house_data %>% 
  ggplot(aes(x=TotalBsmtSF, y=SalePrice)) +
  geom_point()

airP <-
house_data %>% 
  ggplot(aes(x=CentralAir, y=SalePrice)) +
  geom_point()

electricP <-
house_data %>% 
  ggplot(aes(x=Electrical, y=SalePrice)) +
  geom_point()

firstflrsfP <-
house_data %>% 
  ggplot(aes(x=FirstFlrSF, y=SalePrice)) +
  geom_point()

grlivareaP <-
house_data %>% 
  ggplot(aes(x=GrLivArea, y=SalePrice)) +
  geom_point()

bathP <-
house_data %>% 
  ggplot(aes(x=FullBath, y=SalePrice)) +
  geom_point()

rmsabvgrdP <-
house_data %>% 
  ggplot(aes(x=TotRmsAbvGrd, y=SalePrice)) +
  geom_point()

garcarsP <-
house_data %>% 
  ggplot(aes(x=GarageCars, y=SalePrice)) +
  geom_point()

garareaP <-
house_data %>% 
  ggplot(aes(x=GarageArea, y=SalePrice)) +
  geom_point()

```


```{r}
grid.arrange(qualP, buildyearP, remodyearP, bsmtsfP, airP, electricP, firstflrsfP, grlivareaP, bathP, rmsabvgrdP, garcarsP, garareaP,  ncol = 4, newpage = TRUE)
```

I also noticed a few variables which contained many zeroes which likely could have a skewing effect on the data, Enclosed Porch, ThreeSsnPorch, ScreenPorch, and SecondFlrSF. It seems that most of the houses in this dataset do not have porches (so automatically EnclosedPorch, ThreeSsnPorch, and ScreenPorch are not very applicable to our model building.) Weirdly enough, if we look at HouseStyle, we see over 400 houses do have 2 stories but in the SeconfFlrSF variable data, there seems to be a lot of missing measurements of square footage of second floors..So maybe I just won't pay attention to square footage of floors, as there seems to be conflicting data. 

```{r, echo=FALSE}
house_data %>%
  group_by(EnclosedPorch) %>%
  count()

house_data %>%
  group_by(ThreeSsnPorch) %>%
  count()

house_data %>%
  group_by(ScreenPorch) %>%
  count()

house_data %>%
  group_by(SecondFlrSF) %>%
  count
```
```{r}
house_data %>%
  group_by(Fence) %>%
  count


```

```{r, echo=FALSE}
house_data %>%
  group_by(BldgType) %>%
  count()
```

```{r, echo=FALSE}
house_data %>%
  group_by(HouseStyle) %>%
  count()
```

iif rooms vs saleprice is linear plot, dont use factor

#Model 1 - using forward selection
```{r, echo=FALSE}
myvars <- c("OverallQual", "YearBuilt", "YearRemodAdd", "TotalBsmtSF", "CentralAir", "Electrical", "FirstFlrSF", "GrLivArea", "FullBath", "TotRmsAbvGrd", "GarageCars", "GarageArea", "SalePrice")
newhouse_data <- house_data[myvars]
```

```{r, echo=FALSE}
set.seed(399)

House.subset <- 
  newhouse_data %>% 
  mutate(grp = sample(0:1, 
                      size=n(), 
                      replace=TRUE))

training <- House.subset %>% filter(grp == 0)
validation <- House.subset %>% filter(grp == 1)
```

```{r, echo=FALSE}
head(House.subset)
```
Before building a model, I examined the relationship between each included explanatory variable and the response variable (SalePrice) to make sure we could proceed with the linear model assumptions.

```{r, echo=FALSE}
house_data %>%
  group_by(TotalBsmtSF) %>% 
count()
```


```{r, echo=FALSE}
sale <- lm(SalePrice ~ TotalBsmtSF,
                data=house_data)
relate <-
house_data %>% 
  ggplot() +
  geom_jitter(aes(x=TotalBsmtSF, y=SalePrice), width = .2) +
  geom_smooth(aes(x=TotalBsmtSF, y=SalePrice), method = "lm")

distr <-
house_data %>% 
  mutate(fitted.vals = sale$fitted.values, 
         resids = sale$residuals) %>% 
  ggplot(aes(x=resids)) +
  geom_histogram()
```

```{r}
grid.arrange(relate, distr,  ncol = 2, newpage = TRUE)
```

```{r}
bestmods <- regsubsets(x = SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + TotalBsmtSF + CentralAir + FirstFlrSF + GrLivArea + factor(FullBath) + TotRmsAbvGrd + factor(GarageCars) + GarageArea,             
                   data = training %>% 
                     select(-grp),     
                   nvmax = 11,          
                   method = "forward") 
```
When building this model, I initially set the following variables as factor: Electrical, FullBath, TotRmsAbvGrd, and GarageCars. However, when splitting the data into training and validation sets, I faced the problem of too few observations in at least one of the levels. So I chose to represent TotRmsAbvGrd as simply quantitative in my model. I also ended up removing Electrical from my model because there were too few observations in its categorical levels. 
```{r}
house_data %>%
  group_by(Electrical) %>% 
count()
```

```{r, echo=FALSE}
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  coefi <- coef(object, id=id)
  model.matrix(form, data=newdata)[,names(coefi)] %*% coefi
}
```

```{r}
vars <- 11
MSPEval <- c()
for (i in 1:vars){
MSPEval[i] = mean((validation$SalePrice - predict.regsubsets(object = bestmods, 
                   newdata = validation,
                   id = i))^2) 
}
MSPEval
```

Using forward selection, we've chosen the 7 variable model that includes OverallQual, YearBuilt, YearRemodAdd, TotalBsmtSF, GrLivArea, FullBath, and GarageCars.

```{r}
coef(bestmods, id=7)
```
Alternatively, we could have used cross validation AND forward selection.

# Model 2, cross validation AND forward selection
```{r, echo=FALSE}
set.seed(599)

house_data_cv <- house_data %>% 
  mutate(grp = sample(rep(1:5, ceiling(nrow(house_data)/5)), 
                      size=n(), 
                      replace=FALSE))
house_data_cv %>% 
  group_by(grp) %>% 
  count()
```

```{r, echo=FALSE}
grpz <- 5
for (i in 1: grpz){
grp1 <- filter(house_data_cv, grp == i)
show<- grp1 %>% 
  group_by(TotRmsAbvGrd) %>% 
  count()
print(show)}
```
FullBath also had too few observations in each group - some levels were left out, so I removed the variable from my model.

```{r}
test_cv <- house_data_cv %>% filter(grp == i)
validation_cv <- house_data_cv %>% filter(grp != i)
```

For loop time
```{r}
grps <- 5
varz <- 8
MSPE_cv <- matrix(nrow=5, ncol=8)
MSPE_mean <- c()

for (i in 1: grps){
    
test_cv <- house_data_cv %>% filter(grp == i)
training_cv <- house_data_cv %>% filter(grp != i)

bestmods_cv <- regsubsets(x = SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + TotalBsmtSF + CentralAir  + FirstFlrSF + GrLivArea + FullBath + TotRmsAbvGrd + GarageCars + GarageArea,
                   data = training_cv %>%
                     select(-grp),
                   nvmax = 8,
                   method = "forward")
for (j in 1:varz){
  MSPE_cv[i, j] = mean((test_cv$SalePrice - predict.regsubsets(object = bestmods_cv, 
                   newdata = test_cv,
                   id = j))^2) 
  MSPE_mean[j] = mean(MSPE_cv[,j])
}}

MSPE_cv
MSPE_mean
```
The best model with 8 variables had the lowest mean squared prediction error, so we will choose that one.
interp coefficients]

concluding sentences few
---
title: "Predicting Housing Prices"
author: "Lianna Novitz"
date: "3/27/2018"
output: html_document
---

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(ggplot2) #tidyverse
library(dplyr) #tidyverse
library(tidyr) #tidyverse
library(readr) #tidyverse
library(gridExtra)
library(GGally) 
library(leaps) 
library(glmnet)

house_data <- read_csv("https://www.dropbox.com/s/lslekih13vae487/house_train_final.csv?dl=1")

#if you don't need a code chunk at all, {r, eval=FALSE, echo=FALSE}
#if you don't need to show the code but need it evaluated, {r, echo=FALSE}
#won't knit if you have ANY errors, be careful
#don't knit with View(dataset), too big, 
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(1:9,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(10:19,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(20:29,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
#```{r, fig.width=12, fig.height=12}
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(30:39,50)) %>% 
  ggpairs()
```

```{r, echo=FALSE, eval=FALSE}
house_data %>% 
  select(c(40:49,50)) %>% 
  ggpairs()
```

After performing the ggpairs function, I noticed that there are a few variables which have a high correlation with SalePrice. I have graphed these relationships below.
```{r, echo=FALSE}
qualP <-
house_data %>% 
  ggplot(aes(x=OverallQual, y=SalePrice)) +
  geom_point()

buildyearP <-
house_data %>% 
  ggplot(aes(x=YearBuilt, y=SalePrice)) +
  geom_point()

remodyearP <-
house_data %>% 
  ggplot(aes(x=YearRemodAdd, y=SalePrice)) +
  geom_point()

bsmtsfP <-
house_data %>% 
  ggplot(aes(x=TotalBsmtSF, y=SalePrice)) +
  geom_point()

airP <-
house_data %>% 
  ggplot(aes(x=CentralAir, y=SalePrice)) +
  geom_point()

electricP <-
house_data %>% 
  ggplot(aes(x=Electrical, y=SalePrice)) +
  geom_point()

firstflrsfP <-
house_data %>% 
  ggplot(aes(x=FirstFlrSF, y=SalePrice)) +
  geom_point()

grlivareaP <-
house_data %>% 
  ggplot(aes(x=GrLivArea, y=SalePrice)) +
  geom_point()

bathP <-
house_data %>% 
  ggplot(aes(x=FullBath, y=SalePrice)) +
  geom_point()

rmsabvgrdP <-
house_data %>% 
  ggplot(aes(x=TotRmsAbvGrd, y=SalePrice)) +
  geom_point()

garcarsP <-
house_data %>% 
  ggplot(aes(x=GarageCars, y=SalePrice)) +
  geom_point()

garareaP <-
house_data %>% 
  ggplot(aes(x=GarageArea, y=SalePrice)) +
  geom_point()

```


```{r, fig.cap="Figure 1: Variables With Visually High Correlation With Sale Price", echo=FALSE}
grid.arrange(qualP, buildyearP, remodyearP, bsmtsfP, airP, electricP, firstflrsfP, grlivareaP, bathP, rmsabvgrdP, garcarsP, garareaP,  ncol = 4, newpage = TRUE)
```

I also noticed a few variables which contained many zeroes which likely could have a skewing effect on the data, Enclosed Porch, ThreeSsnPorch, ScreenPorch, and SecondFlrSF. It seems that most of the houses in this dataset do not have porches (so automatically EnclosedPorch, ThreeSsnPorch, and ScreenPorch are not very applicable to our model building.) 

Also, if we look at HouseStyle, we see over 400 houses do have 2 stories but in the SeconfFlrSF variable data, there seems to be a lot of missing measurements of square footage of second floors..So I just won't pay attention to square footage of floors, as there seems to be conflicting data. 

```{r, echo=FALSE}
library(knitr)
kable(
  house_data %>% 
    group_by(HouseStyle) %>% 
    count(),
  align = c("l", "c"),
  caption = "Table 1: Distribution of House Style"
)

```

```{r, eval=FALSE, echo=FALSE}
house_data %>%
  group_by(EnclosedPorch) %>%
  count()

house_data %>%
  group_by(ThreeSsnPorch) %>%
  count()

house_data %>%
  group_by(ScreenPorch) %>%
  count()

house_data %>%
  group_by(SecondFlrSF) %>%
  count
```
```{r, eval=FALSE, echo=FALSE}
house_data %>%
  group_by(Fence) %>%
  count
```

```{r, eval=FALSE, echo=FALSE}
house_data %>%
  group_by(BldgType) %>%
  count()
```

```{r, eval=FALSE, echo=FALSE}
house_data %>%
  group_by(HouseStyle) %>%
  count()
```

#Model 1: Predictive Power

From my preliminary exploratory analysis with ggplots, I narrowed down the subset of variables to choose when building our predictive model for SalePrice. We'll look at the following variables: "OverallQual", "YearBuilt", "YearRemodAdd", "TotalBsmtSF", "CentralAir", "Electrical", "FirstFlrSF", "GrLivArea", "FullBath", "TotRmsAbvGrd", "GarageCars", "GarageArea". 
```{r, echo=FALSE}
myvars <- c("OverallQual", "YearBuilt", "YearRemodAdd", "TotalBsmtSF", "CentralAir", "Electrical", "FirstFlrSF", "GrLivArea", "FullBath", "TotRmsAbvGrd", "GarageCars", "GarageArea", "SalePrice")
newhouse_data <- house_data[myvars]
```

First, I divided our dataset into a training and validation set.
```{r, message=FALSE}
set.seed(399)

House.subset <- 
  newhouse_data %>% 
  mutate(grp = sample(0:1, 
                      size=n(), 
                      replace=TRUE))

training <- House.subset %>% filter(grp == 0)
validation <- House.subset %>% filter(grp == 1)
```

```{r, message=FALSE, echo=FALSE}
head(House.subset)
```
Before building a model, I examined the linear relationship between each potential explanatory variable and the response variable (SalePrice) to make sure we could proceed with a linear model. We look for constant variance on the left graph and a relatively normal distribution of residuals on the righthand graph.

```{r, echo=FALSE, eval=FALSE}
house_data %>%
  group_by(TotalBsmtSF) %>% 
count()
```


```{r, echo=FALSE}
sale <- lm(SalePrice ~ GarageArea,
                data=house_data)

relate <-
house_data %>% 
  ggplot() +
  geom_jitter(aes(x=GarageArea, y=SalePrice), width = .2) +
  geom_smooth(aes(x=GarageArea, y=SalePrice), method = "lm")

distr <-
house_data %>% 
  mutate(fitted.vals = sale$fitted.values, 
         resids = sale$residuals) %>% 
  ggplot(aes(x=resids)) +
  geom_histogram()
```

```{r, echo=FALSE}
grid.arrange(relate, distr,  ncol = 2, newpage = TRUE)
```

When building this model, I initially set the following variables as factor: Electrical, FullBath, TotRmsAbvGrd, and GarageCars. However, when splitting the data into training and validation sets, I faced the problem of too few observations in at least one of the levels. So I chose to represent TotRmsAbvGrd as simply quantitative in my model, without factors.

```{r}
bestmods <- regsubsets(x = SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + TotalBsmtSF + CentralAir + FirstFlrSF + GrLivArea + factor(FullBath) + TotRmsAbvGrd + factor(GarageCars) + GarageArea,             
                   data = training %>% 
                     select(-grp),     
                   nvmax = 11,          
                   method = "forward") 
```


```{r, echo=FALSE}
library(knitr)
kable(
  house_data %>% 
    group_by(TotRmsAbvGrd) %>% 
    count(),
  align = c("l", "c"),
  caption = "Table 2: Distribution of Total Rooms Above Ground"
)
```


I also ended up removing Electrical from my model because there were too few observations in its categorical levels. 

```{r, echo=FALSE}
library(knitr)
kable(
  house_data %>% 
    group_by(Electrical) %>% 
    count(),
  align = c("l", "c"),
  caption = "Table 3: Distribution of Electrical System Type"
)

```

```{r, echo=FALSE}
house_data %>%
  group_by(GarageCars) %>% 
count()
```
Having removed inadequate variables, I continued by naming the function which would carry out our forward selection modeling process.
```{r}
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  coefi <- coef(object, id=id)
  model.matrix(form, data=newdata)[,names(coefi)] %*% coefi
}
```
Next, I used the regsubsets forward selection function to find the best model (1 variable or 2 variable ... or 7 variable model) via the lowest mean squared prediction error.
```{r}
vars <- 7
MSPEval <- c()
for (i in 1:vars){
MSPEval[i] = mean((validation$SalePrice - predict.regsubsets(object = bestmods, 
                   newdata = validation,
                   id = i))^2) 
}
MSPEval
```

Using forward selection, we've chosen the 7 variable model that includes OverallQual, YearBuilt, YearRemodAdd, TotalBsmtSF, GrLivArea, FullBath, and GarageCars.

```{r}
coef(bestmods, id=7)
```
Our chosen model is the following:

$$
SalePrice=-1,400,000 + 17,000(OverallQual) + 400(YearBuilt) + \\ 300(YearRemodAdd) + 34(TotalBsmtSF) + 65(GrLivArea) \\ -20,000(FullBath=2) + 43,000(GarageCars = 3)
$$
The coefficients in our model relate to how much the response variable, SalesPrice, changes in regards to its corresponding variable, keeping all other variables constant. 
#
For every point increase in overall quality a house is given, the SalesPrice is expected to increase by  about $17,000, keeping all other variables constant. 

For every year older the house is, the SalesPrice is expected to increase by  about $400, keeping all other variables constant. I would expect the opposite, but no model's perfect.

For every year passed since remodelling, the SalesPrice is expected to increase by  about $300, keeping all other variables constant. Again, I would expect the opposite, but no model's perfect.

For each extra square foot of basement space, the SalesPrice is expected to increase by about $34, keeping all other variables constant. This is almost negligible.

For each increase in living area grade, the SalesPrice is expected to increase by about $65, keeping all other variables constant. Again, this is almost negligible.

Given that there's 2 full bathrooms in a house, the SalesPrice is expected to decrease by about $20,000, on average. 

Given that there's 3 car spaces in the garage, the SalesPrice is expected to increase by about $43,000, on average. 
#

Alternatively, we could have used cross validation AND forward selection to choose our model. I do this next in Model 2.

# Model 2, Interpretability

First, I divided our dataset into 5 groups for cross-validation. We have over 200 observations in each group.
```{r, echo=FALSE, message=FALSE}
set.seed(599)

house_data_cv <- house_data %>% 
  mutate(grp = sample(rep(1:5, ceiling(nrow(house_data)/5)), 
                      size=n(), 
                      replace=FALSE))
```

```{r, echo=FALSE}
kable(
  house_data_cv %>% 
    group_by(grp) %>% 
    count(),
  align = c("l", "c"),
  caption = "Table 4: Distribution of Groups for Cross-Validation"
)
```

```{r, echo=FALSE, eval=FALSE}
grpz <- 5
for (i in 1: grpz){
grp1 <- filter(house_data_cv, grp == i)
show<- grp1 %>% 
  group_by(TotRmsAbvGrd) %>% 
  count()
print(show)}
```

Next, I defined a test set and training set within each fold of our dataset by using a for loop. 
```{r}
grps <- 5
varz <- 6
MSPE_cv <- matrix(nrow=5, ncol=6)
MSPE_mean <- c()

for (i in 1: grps){
    
test_cv <- house_data_cv %>% filter(grp == i)
training_cv <- house_data_cv %>% filter(grp != i)

bestmods_cv <- regsubsets(x = SalePrice ~ OverallQual + YearBuilt + YearRemodAdd + TotalBsmtSF + CentralAir  + FirstFlrSF + GrLivArea + factor(FullBath) + TotRmsAbvGrd + factor(GarageCars) + GarageArea,
                   data = training_cv %>%
                     select(-grp),
                   nvmax = 6,
                   method = "forward")
for (j in 1:varz){
  MSPE_cv[i, j] = mean((test_cv$SalePrice - predict.regsubsets(object = bestmods_cv, 
                   newdata = test_cv,
                   id = j))^2) 
  MSPE_mean[j] = mean(MSPE_cv[,j])
}}

MSPE_mean
```
```{r, echo=FALSE}
kable(
  MSPE_mean, col.names = "Test Error",
  caption = "Table 4: Mean Squared Prediction Errors (1 variable at top, 6 variable at bottom)"
)
```

```{r, echo=FALSE}
coef(bestmods_cv, id=6)
```
Based on the lowest mean squared prediction error (1005433053) after performing 5-fold cross validation and forward selection, we choose the best model with 6 variables.

Our new model is the following:

$$
SalePrice=-1,058,668 + 18,250(OverallQual) + 508(YearBuilt) + 35(TotalBsmtSF) \\ + 62(GrLivArea) -16,669(FullBath = 2) + 35,363(GarageCars = 3)
$$
The coefficients in our model relate to how much the response variable, SalesPrice, changes in regards to its corresponding variable, keeping all other variables constant. 

For every point increase in overall quality a house is given, the SalesPrice is expected to increase by  about $18,000, keeping all other variables constant. 

For every year older the house is, the SalesPrice is expected to increase by  about $500, keeping all other variables constant. I would expect the opposite, but no model's perfect.

For each extra square foot of basement space, the SalesPrice is expected to increase by about $35, keeping all other variables constant. This is almost negligible.

For each increase in living area grade, the SalesPrice is expected to increase by about $62, keeping all other variables constant. This is almost negligible.

Given that there's 2 full bathrooms in a house, the SalesPrice is expected to decrease by about $17,000, on average. 

Given that there's 3 car spaces in the garage, the SalesPrice is expected to increase by about $35,000, on average. 


###Conclusion

Given the models were created using forward selection, our 6-variable Model 2 and 7-variable model Model 1 did not differ that much. The following variables showed up in both models, so are therefore likely to be important variables for predicting house sale prices: OverallQual, YearBuilt, TotalBsmtSF, GrLivArea, FullBath, and GarageCars. In layman's terms, this means overall quality, age, basement space, living area grade, number of bathrooms, and number of garage spaces matters for house prices.